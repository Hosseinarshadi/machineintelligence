{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network implementation with Matrices #4: Parametrised Algorhytm (step by step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been describing the implementation of a neural network with a pre-specified size (the neural network architecture was hard-coded in respect to the number of layers and neurons in each). Here, we will implement an algorithm that can train a neural network of a flexible size in terms of the number of layers and neurons therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, the data and the labels will remain the same as in the previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[ 1.2, 0.7],\n",
    "                 [-0.3,-0.5],\n",
    "                 [ 3.0, 0.1],\n",
    "                 [-0.1,-1.0],\n",
    "                 [-0.0, 1.1],\n",
    "                 [ 2.1,-1.3],\n",
    "                 [ 3.1,-1.8],\n",
    "                 [ 1.1,-0.1],\n",
    "                 [ 1.5,-2.2],\n",
    "                 [ 4.0,-1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([  1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                    -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_data(data, labels):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(data[:,0], data[:,1], c=labels, s=50,  cmap=plt.cm.bwr,zorder=50)\n",
    "    nudge = 0.08\n",
    "    for i, (X,Y) in enumerate(data):\n",
    "        ax.annotate(f'{i}',(X+nudge,Y+nudge))\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 304,
       "width": 316
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of the data in the matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every layer's activation in a neural network is a matrix whose number of rows equals the number of neurons in the layer. Our data should follow the same conventions, such that each data point's row represent an input neurons. Currently our data set represents data points vectors as row vectors. We need to transform them into column vectors, which is done in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.2],\n",
       "        [ 0.7]],\n",
       "\n",
       "       [[-0.3],\n",
       "        [-0.5]],\n",
       "\n",
       "       [[ 3. ],\n",
       "        [ 0.1]],\n",
       "\n",
       "       [[-0.1],\n",
       "        [-1. ]],\n",
       "\n",
       "       [[-0. ],\n",
       "        [ 1.1]],\n",
       "\n",
       "       [[ 2.1],\n",
       "        [-1.3]],\n",
       "\n",
       "       [[ 3.1],\n",
       "        [-1.8]],\n",
       "\n",
       "       [[ 1.1],\n",
       "        [-0.1]],\n",
       "\n",
       "       [[ 1.5],\n",
       "        [-2.2]],\n",
       "\n",
       "       [[ 4. ],\n",
       "        [-1. ]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vectors = np.array([d.reshape(1,2).T for d in data])\n",
    "data_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the cost for a single example we need to subtract the values of the labels from the values of the final activations. For that purpose, it is useful that the labels are also represented as column matrices. We can transform the labels into column vectors with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [0]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_vector = np.array([np.array([1,0]).reshape(1,2).T if l==-1 \n",
    "          else np.array([0,1]).reshape(1,2).T if l==1 \n",
    "          else None \n",
    "          for l in labels])\n",
    "labels_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of the neural network's size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define the variable sizes and use it to define the size of the neural network. The number of elements in the list defines how many layers there are, and the numbers themselves stand for the number of neurons in the current layer. For example, if our network has 3 layers in total: \n",
    "- 2 neurons in the input layer\n",
    "- 3 neurons in the hidden layer and \n",
    "- 2 outputs in the outer layer\n",
    "\n",
    "we can encode it as `[2,3,2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [2,3,2]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the input layer of the neural network is not parametrised by weights and biases. Therefore we need to create bias matrices only for the hidden layer(s) and the output layer. We can select them by `sizes[1:]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bias matrix has only a single column since each neuron has only one bias (its shape is `y`-rows, `1`-columns). This makes it very easy to create a list of all the biases in the network in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.387]\n",
      " [ 0.59 ]\n",
      " [ 0.323]]\n",
      "\n",
      "[[ 1.004]\n",
      " [-0.204]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "# print\n",
    "for b in biases:\n",
    "    print (f'{b}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute all the weights at once we need to know in advance the number of rows and columns in every layer. In the last class we learned that the number of rows in a weight matrix corresponds to the number of neurons in that layer. The number of columns correspond to the number of neurons in the previous layer. To obtain the number of rows (neurons) for each layer except the input layer, we can use `sizes[1:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the number of rows (neurons) of the previous layers in respect to the last operation, we can use `sizes[:-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the sizes of all the weight matrices we only need to combine these two lists together by using `zip` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2:  3 rows, 2 columns ---> (3, 2)\n",
      "layer 3:  2 rows, 3 columns ---> (2, 3)\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(zip(sizes[1:], sizes[:-1])):\n",
    "    print (f'layer {i+2}:  {x} rows, {y} columns ---> {x,y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to create all the weight matrices at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.876  0.898]\n",
      " [-1.403 -1.445]\n",
      " [-1.602 -1.016]]\n",
      "\n",
      "[[-0.899  0.392  0.352]\n",
      " [ 0.28  -0.92   0.357]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [np.random.randn(x, y) for x, y in zip(sizes[1:], sizes[:-1])]\n",
    "# print\n",
    "for w in weights:\n",
    "    print (f'{w}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two variables `weights` and `biases` store all the weights and biases matrices. It would be very useful to store the partial derivatives of weights and biases in the same form. For that we will create two new variables `d_weights` and `d_biases` that will have the same size as their counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biases:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "weights:\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_biases = [np.zeros(b.shape) for b in biases]\n",
    "# print\n",
    "print (\"biases:\")\n",
    "for b in d_biases:\n",
    "    print (f'{b}\\n')\n",
    "    \n",
    "    \n",
    "d_weights = [np.zeros(w.shape) for w in weights]\n",
    "# print\n",
    "print (\"weights:\")\n",
    "for w in d_weights:\n",
    "    print (f'{w}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a forward pass, we will use a single data point, later we will use more points and average their activaitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2],\n",
       "       [0.7]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_point = data_vectors[0]\n",
    "data_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the whole feed forward pass in one go, storing weighted sums and activations within each layer. As before, the data point will be interpreted as the activation of the first (input) layer that is not parametrised by weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the neuron activation will change as we progress through layer\n",
    "# but it starts as a data point in our first layer\n",
    "a = data_point # current activation\n",
    "activations = [data_point] # here we store all the activations, layer by layer\n",
    "weighted_sums = [] # here we store all the weighted sums, layer by layer\n",
    "for b, w in zip(biases, weights):\n",
    "    w_sum = np.dot(w, a)+b #current weighted sum\n",
    "    weighted_sums.append(w_sum) \n",
    "    a = sigmoid(w_sum) # current activation is updated and then the loop goes to the next layer\n",
    "    activations.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print all the values of the weighted sums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.209]\n",
      " [-2.105]\n",
      " [-2.311]]\n",
      "\n",
      "[[ 1.043]\n",
      " [-0.261]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ws in weighted_sums:\n",
    "    print (f'{ws}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print all the activation values layer by layer. **Note that first activation is the same as our data point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2]\n",
      " [0.7]]\n",
      "\n",
      "[[0.039]\n",
      " [0.109]\n",
      " [0.09 ]]\n",
      "\n",
      "[[0.739]\n",
      " [0.435]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in activations:\n",
    "    print (f'{a}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with the data_point with the index `0`, its corresponding label that we will name `current_label` will also have the same index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_label = labels_vector[0]\n",
    "current_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the cost/loss for the current data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost is: 0.28221910236602477\n"
     ]
    }
   ],
   "source": [
    "cost = sum((data_point - activations[-1])**2)[0]\n",
    "print (f\"The cost is: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general strategy is that in the first step we compute the partial derivatives in respect to the weights and biases in the output layer, an in the second step all the rest of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of weights and biases in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the partial derivative of the total cost in respect to the weighted sums of the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.142],\n",
       "       [-0.139]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derivative of the total cost in respect to the output activations \n",
    "# * \n",
    "# derivative of the output activations in respect to the weighted sums\n",
    "d_w_sum = (activations[-1]-current_label) * (activations[-1] * (1-activations[-1])) \n",
    "d_w_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the derivative of the total cost in respect to the weighted sums, it is easy to compute the derivatives of the weights and the biases of the output layer. The derivative of the biases will be the same as the derivatives for the weighted sums, and for the weights, we need to multiply the derivatives of the weighted sums with the transposed activation matrix from the level before the last (output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_biases[-1] = d_w_sum\n",
    "d_weights[-1] = np.dot(d_w_sum, activations[-2].transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we preview the weights and biases matrices, we will see that only the last matrices are filled in, while the rest is still equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[ 0.006  0.015  0.013]\n",
      " [-0.005 -0.015 -0.013]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in d_weights:\n",
    "    print (f'{d}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[ 0.142]\n",
      " [-0.139]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in d_biases:\n",
    "    print (f'{b}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of weights and biases in the hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ought to compute the partial derivatives in respect to the weights and biases of all the layers expect the output layer in one single step. For that, we first determine the number of layers of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of layers is: 3\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(sizes)\n",
    "print (f\"The number of layers is: {num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate through the layers backwards by using the negative value of the iterator `[-i]` starting with the second from the back and ending to the second one from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, num_layers):\n",
    "    act = activations[-i]\n",
    "    d_w_sum = np.dot(weights[-i+1].T, d_w_sum) * (act * (1 - act))\n",
    "    d_biases[-i] = d_w_sum\n",
    "    d_weights[-i] = np.dot(d_w_sum, activations[-i-1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now print the weigths and biases matrices, we will see that all the values are filled in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.007 -0.004]\n",
      " [ 0.021  0.012]\n",
      " [ 0.     0.   ]]\n",
      "\n",
      "[[ 0.006  0.015  0.013]\n",
      " [-0.005 -0.015 -0.013]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in d_weights:\n",
    "    print (f'{d}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.006]\n",
      " [ 0.018]\n",
      " [ 0.   ]]\n",
      "\n",
      "[[ 0.142]\n",
      " [-0.139]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in d_biases:\n",
    "    print (f'{b}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights and biases with we need to subtract the newly computed derivatives from the original values in order to minimise our cost function. In order to minimise the function slowly, we will multiply the values of partial derivatives with a small number, defined with the variable `step_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [w - (step_size * dw) \n",
    "           for w, dw in zip(weights, d_weights)]\n",
    "\n",
    "biases =  [b - (step_size * db)\n",
    "          for b, db in zip(biases, d_biases)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the updated values of the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.007 -0.004]\n",
      " [ 0.021  0.012]\n",
      " [ 0.     0.   ]]\n",
      "\n",
      "[[ 0.006  0.015  0.013]\n",
      " [-0.005 -0.015 -0.013]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in d_weights:\n",
    "    print (f'{d}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.006]\n",
      " [ 0.018]\n",
      " [ 0.   ]]\n",
      "\n",
      "[[ 0.142]\n",
      " [-0.139]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in d_biases:\n",
    "    print (f'{b}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
